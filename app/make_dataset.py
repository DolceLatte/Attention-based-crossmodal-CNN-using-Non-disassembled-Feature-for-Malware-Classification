import torch
from torch.utils.data import Dataset, DataLoader
import numpy as np
from torchvision import transforms
from tqdm import tqdm
import os
from skimage import io
from torch.utils.data import TensorDataset, DataLoader
import pandas as pd
from collections import Counter

class BuildDataLoader():
    def __init__(self,
                 train_fn_entropy=None,
                 train_fn_img=None,
                 fileListPath=None,
                 ):
        MD = MultimodalDataModule(fileListPath,train_fn_img,train_fn_entropy)
        self.trn_loader = MD.train_dataloader()
        self.val_loader = MD.val_dataloader()


class MultimodalDataset(Dataset):
    """Dataset."""
    def __init__(self, fileListPath,fn_img, fn_entropy):
        self.img_data , self.img_y = load_img_data(fileListPath,fn_img)
        self.entropy_data , self.entropy_y ,self.entropy_mask = load_pefile_data(fileListPath,fn_entropy)

    def __len__(self):
        return 10868

    def __getitem__(self, idx):
        entropy_x, entropy_y, mask = self.entropy_data[idx],self.entropy_y[idx],self.entropy_mask[idx]
        img_x, img_y = self.img_data[idx],self.img_y[idx]

        assert entropy_y == img_y

        y = entropy_y

        item = [
            torch.tensor(entropy_x,dtype=torch.float32), #entropy data
            mask, #masking for attn
            torch.tensor(img_x,dtype=torch.float32), #image data
            torch.tensor(y, dtype=torch.int64) #label
        ]
        return item

class MultimodalDataModule(Dataset):
    def __init__(self,
                 fileListPath,
                 fn_img,
                 fn_entropy,
                 batch_size: int = 128):
        super().__init__()
        self.batch_size = batch_size
        self.all_train_dataset = MultimodalDataset(fileListPath,fn_img,fn_entropy)

        # random split train / valiid for early stopping
        N = len(self.all_train_dataset)
        tr = int(N * 0.82)  # 8 for the training
        va = N - tr  # 2 for the validation

        self.train_dataset, self.valid_dataset = torch.utils.data.random_split(self.all_train_dataset, [tr, va],generator=torch.Generator().manual_seed(91))

    def train_dataloader(self):
        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True)  # NOTE : Shuffle

    def val_dataloader(self):
        return DataLoader(self.valid_dataset, batch_size=self.batch_size)


def load_img_data(fileListPath, folder):
    transform = transforms.Compose([
        transforms.ToTensor(),
    ])
    unfoundedFiles = 0
    df = pd.read_csv(fileListPath, sep=',')
    # numpy array, 2D,
    malware_name_label = df.values
    # malware_name_label = df.head(10).values
    mixed_malware_name_label = malware_name_label

    dirTargetHaar2D = "/".join([os.getcwd(), folder])
    filesLen = len(mixed_malware_name_label)
    data_with_padding = np.zeros((filesLen, 64, 784))
    y_label_number = np.zeros(filesLen)
    index = 0

    for entryIndex in tqdm(range(len(mixed_malware_name_label))):
        fetched_name_label = mixed_malware_name_label[entryIndex]
        name_with_extension = fetched_name_label[0] + '.txt.png'
        pathTargetHaar2D = os.path.join(dirTargetHaar2D, name_with_extension)
        try:
            data_non_pad = io.imread(fname=pathTargetHaar2D, as_gray=True)
            data_non_pad = data_non_pad.T
            data_non_pad = transform(data_non_pad)
            data_non_pad = data_non_pad.reshape(64, -1)
            data_with_padding[index] = data_non_pad

            y_label_number[index] = mixed_malware_name_label[entryIndex][1] - 1
            index += 1

        except FileNotFoundError:
            print("File does not exist: " + name_with_extension)
            unfoundedFiles += 1

    if unfoundedFiles != 0:
        print('delete')
        data_with_padding = data_with_padding[: filesLen - unfoundedFiles]
        y_label_number = y_label_number[:filesLen - unfoundedFiles]

    x = data_with_padding
    y = y_label_number
    print(Counter(y))

    return x, y


def load_pefile_data(fileListPath, folder, w=14, MaxChunkLen=3600):
    unfoundedFiles = 0
    df = pd.read_csv(fileListPath, sep=',')
    # numpy array, 2D,
    malware_name_label = df.values
    # malware_name_label = df.head(10).values
    mixed_malware_name_label = malware_name_label

    dirTargetHaar2D = "/".join([os.getcwd(), folder])
    filesLen = len(mixed_malware_name_label)
    data_with_padding = np.zeros((filesLen, MaxChunkLen, w))
    data_with_mask = np.full((filesLen, MaxChunkLen, 1), 1)
    y_label_number = np.zeros(filesLen)
    index = 0

    for entryIndex in tqdm(range(len(mixed_malware_name_label))):
        fetched_name_label = mixed_malware_name_label[entryIndex]
        name_with_extension = fetched_name_label[0] + '.csv'
        pathTargetHaar2D = os.path.join(dirTargetHaar2D, name_with_extension)
        try:
            df_haar = pd.read_csv(pathTargetHaar2D, sep=',', header=None, index_col=None)

            data_non_pad = df_haar.values[:, :w]

            if len(data_non_pad) < MaxChunkLen:
                tp = MaxChunkLen - len(data_non_pad)
                padArray = np.zeros((tp, w))
                padArray_mask = np.zeros((tp, 1))
                data_mask = np.ones((len(data_non_pad),1))

                data_non_pad = np.vstack((data_non_pad, padArray))
                data_mask = np.vstack((data_mask, padArray_mask))

            else:
                data_non_pad = data_non_pad[:MaxChunkLen]
                data_mask = np.ones((len(data_non_pad),1))

            data_with_mask[index] = data_mask
            data_with_padding[index] = data_non_pad

            y_label_number[index] = mixed_malware_name_label[entryIndex][1] - 1
            index += 1

        except FileNotFoundError:
            print("File does not exist: " + name_with_extension)
            unfoundedFiles += 1

    if unfoundedFiles != 0:
        print('delete')
        data_with_padding = data_with_padding[: filesLen - unfoundedFiles]
        y_label_number = y_label_number[:filesLen - unfoundedFiles]

    x = np.transpose(data_with_padding, (0, 2, 1))
    y = y_label_number
    print(Counter(y))
    print(data_with_mask.shape)
    return x, y, data_with_mask

if __name__ == '__main__':
    fileListPath = "../dataset/trainLabels.csv"
    img_folder = "../dataset/malware_img"
    entropy_folder = "../dataset/malware_entropy"

    loader = BuildDataLoader(entropy_folder, img_folder, fileListPath)
